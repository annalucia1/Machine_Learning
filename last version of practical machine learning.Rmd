
---
title: "Practical Machine Learning Project"
author: "Anlu Xing"
date: "August 22, 2015"
output: html_document
---
```{r}
library(doParallel)
registerDoParallel(cores=4)
```

## Introduction

The Weight Lifting Exercises dataset is to investigate "how (well)" an activity was performed by the wearer. Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. This project used data from accelerometers (devices such as Jawbone Up, Nike FuelBand, and Fitbit) on the belt, forearm, arm, and dumbell of 6 participants to quantify how well they do the weight lifting.


### Set up work directory and load libraries and data
```{r}
setwd("C:/Users/anlu/machine-learning")
library(AppliedPredictiveModeling)
library(ggplot2)
library(lattice)
library(caret)
library(rpart)
library(rattle)
library(randomForest)
library(rpart.plot)
set.seed(123)
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
file_dest_training <- "pml-training.csv"
file_dest_testing <- "pml-testing.csv"
#training<- read.csv(file_dest_training, na.strings=c("NA",""), header=TRUE)
#myTesting <-read.csv(file_dest_testing, na.strings=c("NA",""), header=TRUE)
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```

## Clean the Data

### Data cleaning for training set

* Remove near zero covariates

```{r}
nsv <- nearZeroVar(training, saveMetrics = T)
training <- training[, !nsv$nzv]
```

* Remove irrelavent variables in the first 7 columns

```{r}
training <- training[,8:length(colnames(training))]
```

* Clean Variables with too many NAs


```{r}
nav <- sapply(colnames(training), function(x) if(sum(is.na(training[, x])) > 0.6*nrow(training)){return(T)}else{return(F)})
training <- training[, !nav]
```






*Devide data into training set and testing set in order to do out of sample error with cross validation

```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining<-training[inTrain,]
myTesting<-training[-inTrain,]
clean2 <- colnames(myTesting[, -58]) 
myTesting <- myTesting[clean2]
```


## Decision Tree

* Fit model with decision tree algorithm and 3-fold cross validation to predict `classe` with all other predictors.

```{r}
modFitdt <- train(classe ~ ., method = "rpart", data = myTraining,trControl = trainControl(method = "cv", number = 3))
preddt<-predict(modFitdt,myTesting)
confusionMatrix(preddt,myTesting$classe)
confusionMatrix
```
### Out of sample error with cross validation

* From above code, we can see that the out of sample error with cross validation is `r  1-confusionMatrix(preddt,myTesting$classe)$overall[1]` and the accuracy is `r confusionMatrix(preddt,myTesting$classe)$overall[1]`

### Save the results
```{r}
saveRDS(modFitdt, file="modFitdt.rds")
saveRDS(preddt, file="preddt.rds")
saveRDS(confusionMatrix, file="confusiondt.rds")
#modFitrf = readRDS("modFitrf.rds")
```

### The view of decision tree
```{r}
fancyRpartPlot(modFitdt$finalModel)
```

## Random Forests

* Fit model with random forests algorithm and 3-fold cross validation to predict `classe` with all other predictors.The random forests is performed as follows 
```{r}
modFitrf <- train(classe ~ ., method = "rf", data =myTraining, importance = T, trControl = trainControl(method = "cv", number = 3))
predrf<-predict(modFitrf,myTesting)
confusionMatrix(predrf,myTesting$classe)
confusionMatrix
```

### Out of sample error with cross validation

* From above code, we can see that the out of sample error with cross validation is `r  1-confusionMatrix(predrf,myTesting$classe)$overall[1]` and the accuracy is `r confusionMatrix(predrf,myTesting$classe)$overall[1]`

### Most important predictors
```{r}
imp <- varImp(modFitrf)$importance
imp$max <- apply(imp, 1, max)
imp <- imp[order(imp$max, decreasing = T), ]
```
The final random forests model contains 500 trees with 40 variables tried at each split. The five most important predictors in this model are `r rownames(imp)[1:5]`.

### Cache the results

```{r}
saveRDS(modFitrf, file="modFitrf.rds")
saveRDS(predrf, file="predrf.rds")
saveRDS(confusionMatrix, file="confusionrf.rds")
saveRDS(imp, file="imp.rds")
#modFitrf = readRDS("modFitrf.rds")
```

## Conclusion

Compared with decision tree, random forests is a better algrithom with a higher accuracy and lower out of sample error. So I used the random forests predictor to do the final prediction.

## write prediction files

```{r}
prediction<-predict(modFitrf,testing)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_", i, ".txt")
    write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
  }
}
pml_write_files(prediction)
```
